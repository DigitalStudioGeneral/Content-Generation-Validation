{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fMXufE6HyemmlgM9PNBEenBk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120608,
     "status": "ok",
     "timestamp": 1714638781020,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "fMXufE6HyemmlgM9PNBEenBk",
    "outputId": "caeb0592-a185-4720-893d-882048462e6d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install streamlit\n",
    "!pip install streamlit-feedback\n",
    "!pip install --upgrade --quiet  langchain-core \n",
    "!pip install langchain==0.2.1\n",
    "!pip install pypdf\n",
    "!pip install chromadb\n",
    "!pip install transformers\n",
    "!pip install pdf2image\n",
    "!pip install pymupdf\n",
    "!pip install python-docx\n",
    "!pip install python-pptx\n",
    "!pip install openpyxl\n",
    "!pip install PyPDF2\n",
    "!pip install langchain-chroma\n",
    "!pip install -U langchain-community\n",
    "!pip install st-multimodal-chatinput\n",
    "!pip install fpdf2\n",
    "!pip install streamlit-extras\n",
    "%pip install -qU langchain-openai\n",
    "!pip install wget\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42912f99-127c-461e-b1e3-307ef740cf15",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a84401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "start\n",
      "https://content-generation-demo.openai.azure.com/\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     27\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m AzureOpenAIEmbeddings(\n\u001b[1;32m     28\u001b[0m         openai_api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_OPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     29\u001b[0m         azure_endpoint\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_ENDPOINT\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m         dimensions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m,\n\u001b[1;32m     35\u001b[0m         )\n\u001b[1;32m     38\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe meaning of life is 42\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 39\u001b[0m vector \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241m.\u001b[39membed_query(input_text)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(vector[:\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     42\u001b[0m test()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "#Test Embed model:\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embed_model = AzureOpenAIEmbeddings(\n",
    "        openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "\n",
    "        model=os.getenv(\"EMBEDDING_MODEL\"),\n",
    "        azure_deployment=os.getenv(\"EMBEDDING_DEPLOYMENT\"),\n",
    "        openai_api_version=os.getenv(\"EMBEDDING_VERSION\"),\n",
    "        dimensions=768,\n",
    "        )\n",
    "\n",
    "input_text = \"The meaning of life is 42\"\n",
    "vector = embeddings.embed_query(input_text)\n",
    "print(vector[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b584b",
   "metadata": {},
   "source": [
    "# Testing to get the chat models to work:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zM_k1E4KTQ0B",
   "metadata": {
    "id": "zM_k1E4KTQ0B",
    "tags": []
   },
   "source": [
    "##### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd3374a-6095-4a3d-b637-3226814690fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1714638793981,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "5Q0ASgjChiTH",
    "outputId": "638570d5-89ea-4bc6-ebc1-559627db0c26",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streamlit_app_new_experiment1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile streamlit_app_new_experiment1.py\n",
    "#Experiment\n",
    "import streamlit as st\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from typing import Callable\n",
    "from multiprocessing import Pool\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import openai\n",
    "\n",
    "# from langchain_google_vertexai import VertexAI\n",
    "from langchain_openai import AzureOpenAI\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document as Document2 \n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "import vertexai # Change this\n",
    "import pdf2image\n",
    "from pdf2image import convert_from_path\n",
    "from PyPDF2 import PdfReader\n",
    "import docx\n",
    "import pptx\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_google_vertexai import VertexAIEmbeddings #Change this\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "import openpyxl\n",
    "from pptx import Presentation\n",
    "from pptx.enum.shapes import MSO_SHAPE_TYPE\n",
    "from pptx.shapes.placeholder import PlaceholderPicture\n",
    "import sys\n",
    "import fitz\n",
    "import zipfile\n",
    "import shutil\n",
    "from os.path import join\n",
    "from os import listdir, rmdir\n",
    "from shutil import move\n",
    "import pandas as pd\n",
    "from docx.api import Document\n",
    "\n",
    "import base64\n",
    "from langchain_community.chat_models import AzureChatOpenAI # Change this\n",
    "from langchain_core.messages import HumanMessage\n",
    "import uuid\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document as Document1\n",
    "\n",
    "import io\n",
    "from io import BytesIO\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "from st_multimodal_chatinput import multimodal_chatinput\n",
    "from fpdf import FPDF\n",
    "from pathlib import Path\n",
    "from streamlit_extras.stylable_container import stylable_container\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = AzureChatOpenAI (\n",
    "        azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "        openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        \n",
    "        azure_deployment=os.getenv(\"CHAT_DEPLOYMENT\"),\n",
    "        openai_api_version=os.getenv(\"CHAT_VERSION\"),\n",
    "        model_name=os.getenv(\"CHAT_MODEL\"),\n",
    "        temperature=0,\n",
    "        )\n",
    "\n",
    "embed_model = AzureOpenAIEmbeddings(\n",
    "        openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "\n",
    "        model=os.getenv(\"EMBEDDING_MODEL\"),\n",
    "        azure_deployment=os.getenv(\"EMBEDDING_DEPLOYMENT\"),\n",
    "        openai_api_version=os.getenv(\"EMBEDDING_VERSION\"),\n",
    "        dimensions=768,\n",
    "        )\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, stream='no'):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    if stream=='no':\n",
    "        with open(pdf_path, \"rb\", encoding=\"utf-8\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            text = \"\".join(page.extract_text() for page in reader.pages)\n",
    "    if stream=='yes':\n",
    "        text = []\n",
    "        reader = PdfReader(pdf_path)\n",
    "        for page in reader.pages:\n",
    "            text.append(page.extract_text())     \n",
    "    return text\n",
    "\n",
    "def extract_text_from_word(word_path):\n",
    "    \"\"\"Extracts text from a Word document.\"\"\"\n",
    "    doc = docx.Document(word_path)\n",
    "    text = \"\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_ppt(ppt_path):\n",
    "    \"\"\"Extracts text from a PowerPoint presentation.\"\"\"\n",
    "    prs = Presentation(ppt_path)\n",
    "    text = \"\".join(shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, \"text\"))\n",
    "    return text\n",
    "\n",
    "def extract_img_from_pdf(pdf_path, image_root_folder):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    image_folder = image_root_folder\n",
    "    if not os.path.exists(image_folder):\n",
    "        os.makedirs(image_folder)\n",
    "    for page in doc:\n",
    "        pix = page.get_pixmap(matrix=fitz.Identity, dpi=None,\n",
    "                              colorspace=fitz.csRGB, clip=None, alpha=True, annots=True)\n",
    "        pix.save(image_folder + \"/\" + os.path.split(pdf_path)[-1].split(\".\")[0] + \"_%i.png\" % page.number)\n",
    "\n",
    "def extract_img_from_word(word_path, image_root_folder):\n",
    "    archive = zipfile.ZipFile(word_path)\n",
    "    image_folder = image_root_folder\n",
    "    if not os.path.exists(image_folder):\n",
    "        os.makedirs(image_folder)\n",
    "    for file in archive.filelist:\n",
    "        if file.filename.startswith('word/media/') and file.file_size > 30000:\n",
    "            archive.extract(file, image_root_folder)\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    for filename in listdir(join(image_root_folder, 'word/media/')):\n",
    "        move(join(image_root_folder, 'word/media/', filename), join(image_folder + \"/\", os.path.split(word_path)[-1].split(\".\")[0] + \"_\" + filename))\n",
    "    rmdir(join(image_root_folder, 'word/media/'))\n",
    "    rmdir(join(image_root_folder, 'word/'))\n",
    "\n",
    "def write_image(shape, slide_idx, image_idx, ppt_path, image_root_folder):\n",
    "    image_folder = image_root_folder\n",
    "    if not os.path.exists(image_folder):\n",
    "        os.makedirs(image_folder)\n",
    "    image = shape.image\n",
    "    image_bytes = image.blob\n",
    "    image_filename = f'{image_folder}/{os.path.split(ppt_path)[-1].split(\".\")[0]}_slide{slide_idx}_image{image_idx:03d}.{image.ext}'\n",
    "    image_idx += 1\n",
    "    with open(image_filename, 'wb', encoding=\"utf-8\") as f:\n",
    "        f.write(image_bytes)\n",
    "    return image_idx\n",
    "\n",
    "def visitor(shape, slide_idx, image_idx, ppt_path, image_root_folder):\n",
    "    if shape.shape_type == MSO_SHAPE_TYPE.PLACEHOLDER:\n",
    "        if isinstance(shape, PlaceholderPicture):\n",
    "            image_idx = write_image(shape, slide_idx, image_idx, ppt_path, image_root_folder)\n",
    "    if shape.shape_type == MSO_SHAPE_TYPE.GROUP:\n",
    "        for s in shape.shapes:\n",
    "            image_idx = visitor(s, slide_idx, image_idx, ppt_path, image_root_folder)\n",
    "    if shape.shape_type == MSO_SHAPE_TYPE.PICTURE:\n",
    "        image_idx = write_image(shape, slide_idx, image_idx, ppt_path, image_root_folder)\n",
    "    return image_idx\n",
    "\n",
    "def extract_img_from_ppt(ppt_path, image_root_folder):\n",
    "    prs = Presentation(ppt_path)\n",
    "    img_count = 0\n",
    "    for idx, slide in enumerate(prs.slides):\n",
    "        for shape in slide.shapes:\n",
    "            img_count = visitor(shape, idx, img_count, ppt_path, image_root_folder)\n",
    "\n",
    "def extract_table_from_word(word_path):\n",
    "    document = docx.Document(word_path)\n",
    "    tables = document.tables\n",
    "    list_of_tables = []\n",
    "    list_of_data = []\n",
    "\n",
    "    for table in tables:\n",
    "        data = []\n",
    "\n",
    "        for i, row in enumerate(table.rows):\n",
    "            row_data = []\n",
    "\n",
    "            if i == 0:\n",
    "                headers = [cell.text.strip() for cell in row.cells]\n",
    "                continue\n",
    "\n",
    "            for cell in row.cells:\n",
    "                text = cell.text.strip()\n",
    "                row_data.append(text)\n",
    "\n",
    "            if not headers:\n",
    "                row_str = \"\\n\".join(row_data)\n",
    "            else:\n",
    "                row_str = \"\\n\".join([headers[i] + \":\" + row_data[i] for i in range(len(headers))])\n",
    "\n",
    "            data.append(\"\".join(row_str))\n",
    "\n",
    "        list_of_data.append(\"\\n\".join(data))\n",
    "\n",
    "    list_of_tables.append(\"\\n\".join(list_of_data))\n",
    "\n",
    "    return list_of_tables\n",
    "\n",
    "def extract_table_from_pdf(pdf_path):\n",
    "    pdf_file = open(pdf_path, 'rb')\n",
    "    pdf_reader = PdfReader(pdf_file)\n",
    "\n",
    "    table_data = []\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text = page.extract_text()\n",
    "        lines = text.split('\\n')\n",
    "\n",
    "        for line in lines:\n",
    "            table_data.append(line)\n",
    "\n",
    "    return table_data\n",
    "\n",
    "def extract_table_from_ppt(ppt_path):\n",
    "    prs = Presentation((ppt_path))\n",
    "    text_runs = []\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if not shape.has_table:\n",
    "                continue\n",
    "            tbl = shape.table\n",
    "            row_count = len(tbl.rows)\n",
    "            col_count = len(tbl.columns)\n",
    "            for r in range(0, row_count):\n",
    "                for c in range(0, col_count):\n",
    "                    cell = tbl.cell(r, c)\n",
    "                    paragraphs = cell.text_frame.paragraphs\n",
    "                    for paragraph in paragraphs:\n",
    "                        for run in paragraph.runs:\n",
    "                            text_runs.append(run.text)\n",
    "    return text_runs\n",
    "\n",
    "def process_file(file_path, image_root_folder):\n",
    "    text = \"\"\n",
    "    table_data = []\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        extract_img_from_pdf(file_path, image_root_folder)\n",
    "        table_data = extract_table_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\") or file_path.endswith(\".doc\"):\n",
    "        text = extract_text_from_word(file_path)\n",
    "        extract_img_from_word(file_path, image_root_folder)\n",
    "        table_data = extract_table_from_word(file_path)\n",
    "    elif file_path.endswith(\".pptx\") or file_path.endswith(\".ppt\"):\n",
    "        text = extract_text_from_ppt(file_path)\n",
    "        extract_img_from_ppt(file_path, image_root_folder)\n",
    "        table_data = extract_table_from_ppt(file_path)\n",
    "    return text, table_data\n",
    "\n",
    "def process_files(folder_path, save_img):\n",
    "    image_root_folder = os.path.join(save_img, \"images\")\n",
    "    if not os.path.exists(image_root_folder):\n",
    "        os.makedirs(image_root_folder)\n",
    "\n",
    "    file_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path)]\n",
    "\n",
    "    with Pool() as pool:\n",
    "        results = pool.starmap(process_file, [(file_path, image_root_folder) for file_path in file_paths])\n",
    "\n",
    "    texts = [result[0] for result in results]\n",
    "    table_data = [result[1] for result in results]\n",
    "\n",
    "    return texts, table_data\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document2): #\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    if len(b64_images) > 0:\n",
    "        return {\"images\": b64_images[:1], \"texts\": []}\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are digital marketing specialist and you are responsible for either generating content or criticing content based on the user ask.\\n\"\n",
    "            \"Step 1 : You have to identify if task is either for generation of content or validation of content. \\n\"\n",
    "            \"For generation usually user use words like generate, write, create, build, etc. and for Validation user use words like validate, check, approve, justify, give a proof, etc. \\n\"\n",
    "            \"Based on the task, you either follow task step 2a or step 2b and step2b.1. Make sure you only follow one of them not both.\"\n",
    "            \"Step 2a. If task is validation of content, then being digital marketing specialist, provide insight on how much a user given information are following the guidelines given in the context.\"\n",
    "            \"Check all the guideline on that given topic in the context and let the user know in detail which guideline is being followed and which is not.\\n\"\n",
    "            \"Be crticial and specially find out the points that doesn't adhere to given guidelines and sugguest possible improvements.\\n\"\n",
    "            \"Always make sure to additional check that content is using the  tone of voice of Cognizant in naration.\\n\"\n",
    "            \"or \\n\"\n",
    "            \"Step 2b. If task is Generation of content, then Generate content but do remember being digital marketing specialist you are responsible to generate content,\"\n",
    "            \"for the given question and content should adhere to guidelines on that given topic provided in context.\\n\"\n",
    "            \"Always make sure to use tone of voice of Cognizant in naration while generating the content.\\n\"\n",
    "            \"Step 2b.1. Now perform Step 2a. on the result of Step 2b.\"\n",
    "            \"Step 3 : Always provide a score for how well content adhere to guidelines out of 10.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            # print(image_message)\n",
    "            messages.append(image_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "# Multi vector Retriever\n",
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These tables and text are guidelines. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_text)\n",
    "    # model = VertexAI(\n",
    "      # temperature=0, model_name=\"gemini-1.0-pro\", max_output_tokens=1024)#.with_fallbacks([empty_response])\n",
    "    model = llm\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    # model = ChatVertexAI(model_name=\"gemini-1.0-pro-vision\", max_output_tokens=1024)\n",
    "    model = llm\n",
    "\n",
    "    msg = model(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "def process_image(img_path, prompt):\n",
    "    base64_image = encode_image(img_path)\n",
    "    summary = image_summarize(base64_image, prompt)\n",
    "    return base64_image, summary\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for img_file in sorted(os.listdir(path)):\n",
    "            if img_file.endswith(tuple([\".jpg\", \".png\", \".jpeg\"])):\n",
    "                img_path = os.path.join(path, img_file)\n",
    "                future = executor.submit(process_image, img_path, prompt)\n",
    "                futures.append(future)\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            base64_image, summary = future.result()\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(summary)\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document1(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "@st.cache_data\n",
    "def extraction_pipeline(text_list, table_list, save_img):\n",
    "\n",
    "    # model = VertexAI(model_name=\"gemini-1.0-pro\", temperature =0.9)\n",
    "    model = llm\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    context = \"\\n\".join(str(p) for p in text_list)\n",
    "    texts = text_splitter.split_text(context)\n",
    "\n",
    "    # Get text, table summaries\n",
    "    text_summaries, table_summaries = generate_text_summaries(\n",
    "        texts, table_list, summarize_texts=True\n",
    "    )\n",
    "\n",
    "    while '' in text_summaries:\n",
    "        text_summaries.remove('')\n",
    "    while '' in table_summaries:\n",
    "        table_summaries.remove('')\n",
    "\n",
    "    print(\"Text and Table data is ready!!\")\n",
    "\n",
    "    # Image summaries\n",
    "    img_base64_list, image_summaries = generate_img_summaries(f\"./{save_img}\")\n",
    "\n",
    "    print(\"Image data is ready!!\")\n",
    "\n",
    "    print(\"Data is ready!!\")\n",
    "\n",
    "    return text_summaries, table_summaries, texts, table_list, img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    # model = ChatVertexAI(\n",
    "        # temperature=0, model_name=\"gemini-1.0-pro-vision\", max_output_tokens=1024)#1024\n",
    "    model = llm\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    print(\"chain has been created\", chain)\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "def main(retriever_multi_vector_img, username, guidelines_options, selected_guidelines, selected_document):\n",
    "\n",
    "    if \"current_file_index\" not in st.session_state:\n",
    "        st.session_state.current_file_index = 0\n",
    "            \n",
    "    counter = 0\n",
    "    # Check if the username is set\n",
    "    if username and selected_guidelines and selected_document:\n",
    "        \n",
    "        if selected_document == \"I want to generate and validate content according to brand guidelines\":\n",
    "   \n",
    "            chain_multimodal_rag_vg = multi_modal_rag_chain(retriever_multi_vector_img)\n",
    "            print(chain_multimodal_rag_vg)\n",
    "            print(type(chain_multimodal_rag_vg))\n",
    "\n",
    "            output_container = st.container()\n",
    "            chat_history = st.session_state.get(\"chat_history\", [])\n",
    "            if not os.path.exists(\"Chat_history\"):\n",
    "                os.makedirs(\"Chat_history\")\n",
    "            if not os.path.exists(\"user_input_image\"):\n",
    "                os.makedirs(\"user_input_image\")\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            #relative\n",
    "            with stylable_container(\n",
    "                key=\"bottom_content\",\n",
    "                css_styles=\"\"\"\n",
    "                    {\n",
    "                        position: fixed;\n",
    "                        bottom: 5px;\n",
    "                        background-color: #141414;\n",
    "                        width: 100%;\n",
    "                    }\n",
    "                    \"\"\",\n",
    "            ):\n",
    "                st.write(f\"Considering {selected_guidelines} Guidelines\")\n",
    "                st.write(\"You can provide text and images for validation and generation.\")\n",
    "                chatinput = multimodal_chatinput()\n",
    "                \n",
    "            coded_image = []\n",
    "\n",
    "            if chatinput:\n",
    "                uploaded_file = chatinput[\"images\"]\n",
    "                user_input = chatinput[\"text\"]\n",
    "                if uploaded_file:\n",
    "                    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "                            These summaries will be embedded and used to retrieve the raw image. \\\n",
    "                            Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "                    parts = uploaded_file[0].split(',')\n",
    "                    base64_data = parts[1] if len(parts) > 1 else ''\n",
    "                    new_image_summaries = image_summarize(base64_data, prompt)\n",
    "                    user_input = f\"{user_input}. Image information:{new_image_summaries}\"\n",
    "                    coded_image.append(base64_data)\n",
    "\n",
    "                if user_input:\n",
    "                    chat_history.append((\"human\", user_input))\n",
    "                    if coded_image:\n",
    "                        chat_history.append((\"human image\", base64_data))\n",
    "                    # else:\n",
    "                    #     chat_history.append((\"human image\", \"\")) \n",
    "                    response = chain_multimodal_rag_vg.invoke(user_input)\n",
    "                    chat_history.append((\"ai\", response))\n",
    "                    st.session_state[\"chat_history\"] = chat_history\n",
    "                    \n",
    "                    \n",
    "                # with st.container:\n",
    "                pdf = FPDF()\n",
    "                # compression is not yet supported in py3k version\n",
    "                pdf.compress = False\n",
    "                pdf.add_page()\n",
    "                # Unicode is not yet supported in the py3k version; use windows-1252 standard font\n",
    "                pdf.add_font('DejaVu', '', 'DejaVuSans.ttf', uni=True)\n",
    "                pdf.set_font('DejaVu', '', 14)  \n",
    "                pdf.ln(10)\n",
    "                with output_container:\n",
    "                    for i in range(1,len(chat_history)+1):\n",
    "                        sender, message = chat_history[len(chat_history) - i]\n",
    "                        if(sender != \"human image\"):\n",
    "                            # st.markdown(f\"**{sender}:** {message}\")\n",
    "                            st.chat_message(sender).write(message)\n",
    "                        if(sender == \"human image\" and message != \"\"):\n",
    "                            st.markdown(f\"\"\"<img src=\"data:png;base64,{message}\" width='50' height='50' >\"\"\", True)\n",
    "                        if(i < 3):\n",
    "                            if(sender != \"human image\"):\n",
    "                                pdf.write(5, f\"**{sender}:** {message}\")\n",
    "                            else: \n",
    "                                if(len(coded_image) != 0 and i==2):\n",
    "                                    image_bytes = base64.b64decode(coded_image[0])\n",
    "                                    image = Image.open(BytesIO(image_bytes))\n",
    "                                    image.save(f\"user_input_image/{username}_{counter}.png\", format=\"PNG\")\n",
    "                                    pdf.add_page()\n",
    "                                    pdf.image(f\"user_input_image/{username}_{counter}.png\", 10, 10)\n",
    "                                    counter += 1\n",
    "                    pdf.output(f'Chat_history/Chat_history_{username}.pdf', 'F')\n",
    "                    print(\"created\")\n",
    "                \n",
    "        elif selected_document == \"I want to validate a document content according to brand guidelines\":\n",
    "            chain_multimodal_rag_vg = multi_modal_rag_chain(retriever_multi_vector_img)\n",
    "            output_doc_container = st.container()\n",
    "            st.session_state[\"chat_history\"] = []\n",
    "            chat_history = st.session_state.get(\"chat_history\", [])\n",
    "            if not os.path.exists(\"pdf_by_user\"):\n",
    "                os.makedirs(\"pdf_by_user\")\n",
    "            \n",
    "            if \"query\" not in st.session_state:\n",
    "                st.session_state[\"query\"] = \"\"\n",
    "            \n",
    "            if 'clicked' not in st.session_state:\n",
    "                st.session_state.submit = False\n",
    "            # with st.container():\n",
    "            with stylable_container(\n",
    "                key=\"bottom_content_document\",\n",
    "                css_styles=\"\"\"\n",
    "                    {\n",
    "                        position: fixed;\n",
    "                        bottom: 5px;\n",
    "                        background-color: #141414;\n",
    "                        width: 100%\n",
    "                        \n",
    "                        }\n",
    "                        \n",
    "                    [data-testid='stFileUploader'] {\n",
    "                            width: max-content;\n",
    "                            }\n",
    "                    \"\"\",\n",
    "            ):\n",
    "                col1, col2 = st.columns([2,4]) \n",
    "                with col1:\n",
    "                    st.write(f\"Considering {selected_guidelines} Guidelines\")\n",
    "                # st.write(\"You can provide pdf document for validation.\")\n",
    "                    query = st.text_input(\"Anything specific you want to validate in the pdf?(optional)\", key='doc_val', value=st.session_state[\"query\"])\n",
    "                with col2:\n",
    "                    file = st.file_uploader(\"You can provide pdf document for validation.\", \"pdf\")#Please choose a pdf file\n",
    "                doc_submit = st.button(\"Submit\")#, on_click= st.session_state.submit = True)\n",
    "\n",
    "                \n",
    "            \n",
    "            if doc_submit:\n",
    "            \n",
    "                if file != None:\n",
    "                    chat_history.append((\"human\", file.name))\n",
    "                    text = extract_text_from_pdf(file, stream='yes')\n",
    "                    save_folder = 'pdf_by_user'\n",
    "                    save_path = Path(save_folder, file.name)\n",
    "                    with open(save_path, mode='wb') as w:\n",
    "                        w.write(file.getvalue())\n",
    "\n",
    "                    if save_path.exists():\n",
    "                        st.success(f'File {file.name} is successfully saved!')\n",
    "\n",
    "\n",
    "\n",
    "                    extract_img_from_pdf(f\"{save_folder}/{file.name}\", file.name.split('.', 1)[0])\n",
    "                    print(\"images extracted\")\n",
    "                    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "                                These summaries will be embedded and used to retrieve the raw image. \\\n",
    "                                Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "                    count = 0\n",
    "                    with st.spinner('Wait for it...'):\n",
    "                        for img_file in sorted(os.listdir(file.name.split('.', 1)[0])):\n",
    "                            if img_file.endswith(tuple([\".jpg\", \".png\", \".jpeg\"])):\n",
    "                                img_path = os.path.join(file.name.split('.', 1)[0], img_file)\n",
    "                                base64_image = encode_image(img_path)\n",
    "                                image_summaries = image_summarize(base64_image, prompt)\n",
    "                                user_input = f'Can you validate this information. Query : {query}, Textual information : {text[count]} , Image Information : {image_summaries}'\n",
    "                                response = chain_multimodal_rag_vg.invoke(user_input)\n",
    "                                count += 1\n",
    "                                chat_history.append((\"ai\", f\"**Page {count}**, {response}\"))\n",
    "                                st.session_state[\"chat_history\"] = chat_history\n",
    "                    st.success('Done!')\n",
    "                    \n",
    "                    with output_doc_container:\n",
    "                        for sender, message in chat_history:\n",
    "                            print(chat_history)\n",
    "                            st.chat_message(sender).write(message)\n",
    "                else:\n",
    "                    st.warning(\"Please upload a document.\")\n",
    "                    \n",
    "\n",
    "                        \n",
    "    else:\n",
    "        st.warning(\"Please enter your name and select both the fields to enable the app functionality.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "    <style>\n",
    "        .st-emotion-cache-4oy321 {\n",
    "            flex-direction: row-reverse;\n",
    "            text-align: right;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\",\n",
    "        unsafe_allow_html=True,\n",
    "    )\n",
    "    \n",
    "    guidelines_options = [\"Website\", \"ExternalComms\", \"SocialMedia\", \"Emails\"]\n",
    "\n",
    "    if \"username\" not in st.session_state:\n",
    "        st.session_state[\"username\"] = \"\"\n",
    "        \n",
    "    sibebar_submit =False\n",
    "    \n",
    "    with st.sidebar:\n",
    "        st.write(\"Just Create\")\n",
    "        with st.container():\n",
    "            username = st.text_input(\"Enter Your Name\", key='username', value=st.session_state[\"username\"])\n",
    "            document_options = [\"I want to generate and validate content according to brand guidelines\",\"I want to validate a document content according to brand guidelines\"]\n",
    "            selected_document  = st.radio(\"What do you want help with?\", document_options, key=\"document_w_selection\", index= None)\n",
    "            guidelines_options = [\"Website\", \"ExternalComms\", \"SocialMedia\", \"Emails\"]\n",
    "            selected_guidelines  = st.radio(\"Which guideline should we consider?\", guidelines_options, key=\"guidelines_selection\", index= None)\n",
    "            \n",
    "         \n",
    "    if username and selected_guidelines and selected_document:\n",
    "        vectordb = Chroma(persist_directory=f\"./chroma/chroma_guidelines_{selected_guidelines}\", embedding_function=embed_model)\n",
    "        folder_path = f\"Guidelines_subfolders/{selected_guidelines}\"\n",
    "        save_img = f\"imgs_guidelines/imgs_guidelines_{selected_guidelines}\"\n",
    "        guidelines = f\"Guidelines_subfolders/{selected_guidelines}\"\n",
    "        save_img_folder = f\"imgs_guidelines/imgs_guidelines_{selected_guidelines}\"\n",
    "\n",
    "        if selected_guidelines == \"Website\":\n",
    "            if os.path.exists(f\"./chroma/chroma_guidelines_{selected_guidelines}\"): \n",
    "                  retriever_multi_vector_img = vectordb.as_retriever()\n",
    "            else:\n",
    "                text_list_Website, table_list_Website = process_files(folder_path, save_img)\n",
    "                text_summaries_Website, table_summaries_Website, texts_Website, table_list_Website, img_base64_list_Website, image_summaries_Website = extraction_pipeline(text_list_Website, table_list_Website, save_img_folder)\n",
    "\n",
    "\n",
    "                retriever_multi_vector_img = create_multi_vector_retriever(vectordb, text_summaries_Website,texts_Website,\n",
    "                                                                    table_summaries_Website,table_list_Website,\n",
    "                                                                    image_summaries_Website,img_base64_list_Website,)\n",
    "        elif selected_guidelines == \"ExternalComms\":\n",
    "\n",
    "            if os.path.exists(f\"./chroma/chroma_guidelines_{selected_guidelines}\"):\n",
    "                retriever_multi_vector_img = vectordb.as_retriever()\n",
    "            else:\n",
    "                text_list_ExternalComms, table_list_ExternalComms = process_files(folder_path, save_img)\n",
    "                text_summaries_ExternalComms, table_summaries_ExternalComms, texts_ExternalComms, table_list_ExternalComms, img_base64_list_ExternalComms, image_summaries_ExternalComms = extraction_pipeline(text_list_ExternalComms, table_list_ExternalComms, save_img_folder)\n",
    "                retriever_multi_vector_img = create_multi_vector_retriever(vectordb, text_summaries_ExternalComms,texts_ExternalComms,\n",
    "                                                                    table_summaries_ExternalComms,table_list_ExternalComms,\n",
    "                                                                    image_summaries_ExternalComms,img_base64_list_ExternalComms,)\n",
    "        elif selected_guidelines == \"SocialMedia\":\n",
    "\n",
    "            if os.path.exists(f\"./chroma/chroma_guidelines_{selected_guidelines}\"):\n",
    "                retriever_multi_vector_img = vectordb.as_retriever()\n",
    "            else:\n",
    "                text_list_SocialMedia, table_list_SocialMedia = process_files(folder_path, save_img)\n",
    "                text_summaries_SocialMedia, table_summaries_SocialMedia, texts_SocialMedia, table_list_SocialMedia, img_base64_list_SocialMedia, image_summaries_SocialMedia = extraction_pipeline(text_list_SocialMedia, table_list_SocialMedia, save_img_folder)\n",
    "                retriever_multi_vector_img = create_multi_vector_retriever(vectordb, text_summaries_SocialMedia,texts_SocialMedia,\n",
    "                                                                    table_summaries_SocialMedia,table_list_SocialMedia,\n",
    "                                                                    image_summaries_SocialMedia,img_base64_list_SocialMedia,)\n",
    "        elif selected_guidelines == \"Emails\":\n",
    "\n",
    "            if os.path.exists(f\"./chroma/chroma_guidelines_{selected_guidelines}\"):\n",
    "                retriever_multi_vector_img = vectordb.as_retriever()\n",
    "            else: \n",
    "                text_list_Emails, table_list_Emails = process_files(folder_path, save_img)\n",
    "                text_summaries_Emails, table_summaries_Emails, texts_Emails, table_list_Emails, img_base64_list_Emails, image_summaries_Emails = extraction_pipeline(text_list_Emails, table_list_Emails, save_img_folder)\n",
    "                retriever_multi_vector_img = create_multi_vector_retriever(vectordb, text_summaries_Emails,texts_Emails,\n",
    "                                                                    table_summaries_Emails,table_list_Emails,\n",
    "                                                                    image_summaries_Emails,img_base64_list_Emails,)\n",
    "\n",
    "        \n",
    "\n",
    "        if retriever_multi_vector_img:\n",
    "            main(retriever_multi_vector_img, username,guidelines_options,selected_guidelines, selected_document)\n",
    "    else:\n",
    "        st.title(\"Just Create\")\n",
    "        st.write(\"Welcome to Just Create!\")\n",
    "        st.write(\"With this AI powered solution you can generate and validate content according to brand guidelines in simple steps.\")\n",
    "        st.write(\"You can generate textual content just by asking with support of text and images.\")\n",
    "        st.write(\"If you want to validate content, you can provide text or image to validate or upload th PDF document itself.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e115cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8502\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.20.198.94:8502\u001b[0m\n",
      "\u001b[0m\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!streamlit run streamlit_app_new_experiment1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e17d99-7fbc-467e-b801-8c08dc536d71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1714638796870,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "I5svDuKGmpse",
    "outputId": "5c67a59e-e0eb-46bb-8ee4-7b9cf7555c27",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!wget -q -O - ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19f324-c329-4ccb-92a9-6d6f345964ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1840993,
     "status": "ok",
     "timestamp": 1714640845954,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "MAvm4j1ZCTRC",
    "outputId": "292268af-fb47-4a11-b614-010dd105293c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!streamlit run streamlit_app_new_experiment1.py --server.enableCORS=false --server.enableXsrfProtection=false --browser.gatherUsageStats=false & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78f53b-d51d-4640-adb2-24f2dfdb9339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a613b-90d3-4309-8077-06dbf2b2eed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eW8iAQ0Y-2Mp",
    "eLhAgdewTGd5"
   ],
   "name": "Eval_streamlit_app_payanshi.jain (Apr 12, 2024, 3:56:03PM)",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m116"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
